{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e927453b",
   "metadata": {},
   "source": [
    "## Подключение всех необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "659dba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import torchaudio\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae32041f",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2194112c",
   "metadata": {},
   "source": [
    "## Extracting and saving features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86303d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(\"normal_audio\"):\n",
    "    y, sr = librosa.load(os.path.join(\"normal_audio\", file))\n",
    "    if len(y) == 0:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3175f6d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normal_audio:   0%|          | 0/15166 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normal_audio: 100%|██████████| 15166/15166 [00:43<00:00, 347.27it/s]\n",
      "burr_audio: 100%|██████████| 7818/7818 [00:25<00:00, 307.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from extract_R import extract_R\n",
    "#extract_R()\n",
    "for folder in [\"normal_audio\", \"burr_audio\"]:\n",
    "    for filename in tqdm.tqdm(os.listdir(folder), desc=folder): \n",
    "        audio, sr = librosa.load(os.path.join(folder, filename), sr=16000)\n",
    "        mel = librosa.feature.melspectrogram(y=audio,sr=sr)\n",
    "        mel_df = pd.DataFrame(mel)\n",
    "        mel_df.to_csv(os.path.join(folder + \"_mel\", filename + \"_mel.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "64ba8f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2870 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2870/2870 [03:29<00:00, 13.68it/s]\n"
     ]
    }
   ],
   "source": [
    "for filename in tqdm.tqdm(os.listdir(\"test_audio_raw\")):\n",
    "    audio, sr = librosa.load(os.path.join(\"test_audio_raw\", filename), sr=22050)\n",
    "    audio = librosa.resample(audio, orig_sr=22050, target_sr=16000)\n",
    "    if audio.size % 4410 != 0:\n",
    "        audio = audio[:-int(audio.size % 4410)]\n",
    "    if not os.path.exists(os.path.join(\"test_audio_mel\", filename)):\n",
    "        os.mkdir(os.path.join(\"test_audio_mel\", filename))\n",
    "    splits = np.split(audio, audio.size // 4410)\n",
    "    for i, split in enumerate(splits):\n",
    "        mel = librosa.feature.melspectrogram(y=split,sr=16000)\n",
    "        df = pd.DataFrame(mel)\n",
    "        df.to_csv(os.path.join(\"test_audio_mel\", filename, f\"mel_{i}.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db8d5ea",
   "metadata": {},
   "source": [
    "## Loading extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3b394888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normal_audio_mel: 100%|██████████| 15166/15166 [00:10<00:00, 1459.63it/s]\n",
      "burr_audio_mel: 100%|██████████| 7818/7818 [00:05<00:00, 1435.37it/s]\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "X = []\n",
    "y = []\n",
    "for folder in [\"normal_audio_mel\", \"burr_audio_mel\"]:\n",
    "    for mel in tqdm.tqdm(os.listdir(folder), desc=folder):    \n",
    "        df = pd.read_csv(os.path.join(folder, mel), index_col=0)\n",
    "        X.append(df.values)\n",
    "        y.append(folder == \"burr_audio_mel\")\n",
    "X = torch.tensor(X).unsqueeze(1)\n",
    "y = torch.tensor(y).unsqueeze(1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "289a3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        return self.features[ind], self.labels[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "43f7185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "train_dataset = MyDataset(x_train, y_train)\n",
    "test_dataset = MyDataset(x_test, y_test)\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6718c",
   "metadata": {},
   "source": [
    "# Preparing our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef5e0c",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "43c860d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_conv2d_out_shape(input_shape,conv: nn.Conv2d,pool=2):\n",
    "    # get conv arguments\n",
    "    win, hin = input_shape\n",
    "    kernel_size=conv.kernel_size\n",
    "    stride=conv.stride\n",
    "    padding=conv.padding\n",
    "    dilation=conv.dilation\n",
    "    if not isinstance(padding, tuple):\n",
    "        padding = (padding,padding)\n",
    "    if not isinstance(stride, tuple):\n",
    "        stride = (stride,stride)\n",
    "    if not isinstance(dilation, tuple):\n",
    "        dilation = (dilation,dilation)\n",
    "    if not isinstance(kernel_size, tuple):\n",
    "        kernel_size = (kernel_size,kernel_size)\n",
    "    hout=np.floor((hin \n",
    "                   + 2*padding[0] \n",
    "                   - dilation[0] * (kernel_size[0]-1) - 1)\n",
    "                   / stride[0] + 1)\n",
    "    wout=np.floor((win+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1]+1)\n",
    "\n",
    "    if pool:\n",
    "        hout/=pool[0]\n",
    "        wout/=pool[1]\n",
    "    return int(hout),int(wout)\n",
    "\n",
    "\n",
    "class MelConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MelConvNet, self).__init__()\n",
    "        input_shape = (129, 9)\n",
    "        out_ch = 3\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, out_ch, kernel_size=(3,3), stride=(1,1), padding=(0,0))\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        width, height = find_conv2d_out_shape(input_shape, self.conv1, pool=self.pool1.kernel_size)\n",
    "         \n",
    "        # self.conv2 = nn.Conv2d(16, out_ch, (3,3), (1,1), (0,0))\n",
    "        # self.pool2 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        # width, height = find_conv2d_out_shape(width, height, self.conv2)\n",
    "         \n",
    "        # self.conv3 = nn.Conv2d(16, out_ch, (3,3), (1,1), (0,0))\n",
    "        # self.pool3 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        # width, height = find_conv2d_out_shape(width, height, self.conv3)\n",
    "\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc = nn.Linear(width * height * out_ch, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "         \n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "         \n",
    "        # x = self.conv2(x)\n",
    "        # x = self.pool2(x)\n",
    "         \n",
    "        # x = self.conv3(x)\n",
    "        # x = self.pool3(x)\n",
    "         \n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a8ddc2",
   "metadata": {},
   "source": [
    "## Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "56dcd183",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MelConvNet()\n",
    "model.to(device)\n",
    "total_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "75c66a49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loader: val. Accuracy: 61.69%:   9%|▉         | 93/1000 [03:12<31:20,  2.07s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[213], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m total \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     30\u001b[0m preds \u001b[38;5;241m=\u001b[39m outp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m---> 31\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m all_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(outp)\n\u001b[1;32m     33\u001b[0m epoch_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m correct\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "lr = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loaders = {\"train\": train_loader, \"val\": test_loader}\n",
    "accuracy = {\"train\": [], \"val\": []}\n",
    "val_progress = 0 # track accuracy improvement or degradation\n",
    "VAL_DECREASE_BREAK = -epochs # when decreased x times, model overfitted, stop training\n",
    "model.train()\n",
    "pbar = tqdm.tqdm(range(epochs))\n",
    "for epoch in pbar:\n",
    "    for key, loader in loaders.items():\n",
    "        epoch_correct = 0\n",
    "        epoch_all = 0\n",
    "        for x_batch, y_batch in loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            if key == \"train\":\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                outp = model(x_batch)\n",
    "                loss = loss_fn(outp, y_batch.squeeze(1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    outp = model(x_batch)\n",
    "            total = y_batch.size(0)\n",
    "            preds = outp > 0.5\n",
    "            correct = (preds == y_batch).sum().item()\n",
    "            all_ = len(outp)\n",
    "            epoch_correct += correct\n",
    "            epoch_all += all_\n",
    "        pbar.set_description(f\"Loader: {key}. Accuracy: {epoch_correct/epoch_all:.2%}\")\n",
    "        accuracy[key].append(epoch_correct/epoch_all)\n",
    "        if key == \"val\" and len(accuracy[key]) >= 2:\n",
    "            if val_progress > 0:\n",
    "                val_progress = 0\n",
    "            if accuracy[key][-1] > accuracy[key][-2]:\n",
    "                val_progress += 1\n",
    "            else:\n",
    "                val_progress -= 1\n",
    "            if val_progress <= VAL_DECREASE_BREAK:\n",
    "                print(f\"OVERFITTED AT EPOCH {epoch}\")\n",
    "                break\n",
    "    else: # if not broken with val decrease excision\n",
    "        total_epochs += 1\n",
    "        continue\n",
    "    break\n",
    "[plt.plot(accuracies, label=name) for name, accuracies in accuracy.items()]\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c33b8",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f61aaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2870/2870 [00:31<00:00, 90.25it/s] \n"
     ]
    }
   ],
   "source": [
    "data_scoring = []\n",
    "paths = []\n",
    "for mels_path in tqdm.tqdm(os.listdir(\"test_audio_mel\")): \n",
    "    paths.append(mels_path) \n",
    "    mels_pack = []\n",
    "    for mel_path in os.listdir(os.path.join(\"test_audio_mel\", mels_path)): \n",
    "        df = pd.read_csv(os.path.join(\"test_audio_mel\", mels_path, mel_path), header=None, index_col=0)\n",
    "        mels_pack.append(df.values)\n",
    "    data_scoring.append(mels_pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "938a10ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2870/2870 [00:23<00:00, 122.83it/s]\n"
     ]
    }
   ],
   "source": [
    "sample_submission = pd.read_csv(\"test (1).csv\")\n",
    "print(\"predicting..\")\n",
    "CONFIDENCE = 0.7\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for mel_pack in tqdm.tqdm(data_scoring):\n",
    "        mel_pack = torch.tensor(mel_pack, dtype=torch.float32).to(device).unsqueeze(1)\n",
    "        results = model(mel_pack)\n",
    "        confident = ~((results < CONFIDENCE) & (results > 1 - CONFIDENCE))\n",
    "        results = results[confident]\n",
    "        predicted = (results > 0.5)\n",
    "        label = predicted.sum() > predicted.size()[0] / 2\n",
    "        predictions.append(label.item())\n",
    "\n",
    "\n",
    "submission = pd.DataFrame({\"Filename\": paths, \"Label\": predictions})\n",
    "submission = submission.sort_values(by=\"Filename\")\n",
    "sample_submission = sample_submission.sort_values(by=\"Filename\")\n",
    "sample_submission[\"Label\"] = submission[\"Label\"]\n",
    "sample_submission = sample_submission.sort_index()\n",
    "sample_submission.to_csv(\"submission.csv\", index=False, header=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
