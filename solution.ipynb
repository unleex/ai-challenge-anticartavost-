{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e927453b",
   "metadata": {},
   "source": [
    "## Подключение всех необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "659dba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import torchaudio\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae32041f",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86303d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(\"normal_audio\"):\n",
    "    y, sr = librosa.load(os.path.join(\"normal_audio\", file))\n",
    "    if len(y) == 0:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3175f6d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normal_audio:   0%|          | 0/15166 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normal_audio: 100%|██████████| 15166/15166 [00:43<00:00, 347.27it/s]\n",
      "burr_audio: 100%|██████████| 7818/7818 [00:25<00:00, 307.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from extract_R import extract_R\n",
    "#extract_R()\n",
    "for folder in [\"normal_audio\", \"burr_audio\"]:\n",
    "    for filename in tqdm.tqdm(os.listdir(folder), desc=folder): \n",
    "        audio, sr = librosa.load(os.path.join(folder, filename), sr=16000)\n",
    "        mel = librosa.feature.melspectrogram(y=audio,sr=sr)\n",
    "        mel_df = pd.DataFrame(mel)\n",
    "        mel_df.to_csv(os.path.join(folder + \"_mel\", filename + \"_mel.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db8d5ea",
   "metadata": {},
   "source": [
    "## Loading extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b394888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normal_audio_mel: 100%|██████████| 15166/15166 [00:09<00:00, 1542.55it/s]\n",
      "burr_audio_mel: 100%|██████████| 7818/7818 [00:04<00:00, 1613.71it/s]\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"mps\")\n",
    "X = []\n",
    "y = []\n",
    "for folder in [\"normal_audio_mel\", \"burr_audio_mel\"]:\n",
    "    for mel in tqdm.tqdm(os.listdir(folder), desc=folder):    \n",
    "        df = pd.read_csv(os.path.join(folder, mel), index_col=0)\n",
    "        X.append(df.values)\n",
    "        y.append(folder == \"burr_audio_mel\")\n",
    "X = torch.tensor(X).unsqueeze(1)\n",
    "y = torch.tensor(y).unsqueeze(1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "289a3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        return self.features[ind], self.labels[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "43f7185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "train_dataset = MyDataset(x_train, y_train)\n",
    "test_dataset = MyDataset(x_test, y_test)\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6718c",
   "metadata": {},
   "source": [
    "# Preparing our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef5e0c",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "43c860d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_conv2d_out_shape(input_shape,conv: nn.Conv2d,pool=2):\n",
    "    # get conv arguments\n",
    "    win, hin = input_shape\n",
    "    kernel_size=conv.kernel_size\n",
    "    stride=conv.stride\n",
    "    padding=conv.padding\n",
    "    dilation=conv.dilation\n",
    "    if not isinstance(padding, tuple):\n",
    "        padding = (padding,padding)\n",
    "    if not isinstance(stride, tuple):\n",
    "        stride = (stride,stride)\n",
    "    if not isinstance(dilation, tuple):\n",
    "        dilation = (dilation,dilation)\n",
    "    if not isinstance(kernel_size, tuple):\n",
    "        kernel_size = (kernel_size,kernel_size)\n",
    "    hout=np.floor((hin \n",
    "                   + 2*padding[0] \n",
    "                   - dilation[0] * (kernel_size[0]-1) - 1)\n",
    "                   / stride[0] + 1)\n",
    "    wout=np.floor((win+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1]+1)\n",
    "\n",
    "    if pool:\n",
    "        hout/=pool[0]\n",
    "        wout/=pool[1]\n",
    "    return int(hout),int(wout)\n",
    "\n",
    "\n",
    "class MelConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MelConvNet, self).__init__()\n",
    "        input_shape = (129, 9)\n",
    "        out_ch = 3\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, out_ch, kernel_size=(3,3), stride=(1,1), padding=(0,0))\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        width, height = find_conv2d_out_shape(input_shape, self.conv1, pool=self.pool1.kernel_size)\n",
    "         \n",
    "        # self.conv2 = nn.Conv2d(16, out_ch, (3,3), (1,1), (0,0))\n",
    "        # self.pool2 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        # width, height = find_conv2d_out_shape(width, height, self.conv2)\n",
    "         \n",
    "        # self.conv3 = nn.Conv2d(16, out_ch, (3,3), (1,1), (0,0))\n",
    "        # self.pool3 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        # width, height = find_conv2d_out_shape(width, height, self.conv3)\n",
    "\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc = nn.Linear(width * height * out_ch, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "         \n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "         \n",
    "        # x = self.conv2(x)\n",
    "        # x = self.pool2(x)\n",
    "         \n",
    "        # x = self.conv3(x)\n",
    "        # x = self.pool3(x)\n",
    "         \n",
    "        x = self.flatten(x)\n",
    "         \n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a8ddc2",
   "metadata": {},
   "source": [
    "## Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "56dcd183",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MelConvNet()\n",
    "model.to(device)\n",
    "total_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f2110ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 128, 9])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "75c66a49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loader: val. Accuracy: 66.13%:  36%|███▌      | 355/1000 [14:58<26:48,  2.49s/it]  "
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "lr = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "loss_fn = nn.BCELoss()\n",
    "loaders = {\"train\": train_loader, \"val\": test_loader}\n",
    "accuracy = {\"train\": [], \"val\": []}\n",
    "val_progress = 0 # track accuracy improvement or degradation\n",
    "VAL_DECREASE_BREAK = -epochs # when decreased x times, model overfitted, stop training\n",
    "model.train()\n",
    "pbar = tqdm.tqdm(range(epochs))\n",
    "for epoch in pbar:\n",
    "    for key, loader in loaders.items():\n",
    "        epoch_correct = 0\n",
    "        epoch_all = 0\n",
    "        for x_batch, y_batch in loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            if key == \"train\":\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                outp = model(x_batch)\n",
    "                loss = loss_fn(outp, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    outp = model(x_batch)\n",
    "            total = y_batch.size(0)\n",
    "            predicted = outp.data >= 0.5\n",
    "            correct = (predicted == y_batch).sum().item()\n",
    "            correct = sum(predicted == y_batch)\n",
    "            all_ = len(outp)\n",
    "            epoch_correct += correct.item()\n",
    "            epoch_all += all_\n",
    "        #scheduler.step()\n",
    "        pbar.set_description(f\"Loader: {key}. Accuracy: {epoch_correct/epoch_all:.2%}\")\n",
    "        accuracy[key].append(epoch_correct/epoch_all)\n",
    "        if key == \"val\" and len(accuracy[key]) >= 2:\n",
    "            if val_progress > 0:\n",
    "                val_progress = 0\n",
    "            if accuracy[key][-1] > accuracy[key][-2]:\n",
    "                val_progress += 1\n",
    "            else:\n",
    "                val_progress -= 1\n",
    "            if val_progress <= VAL_DECREASE_BREAK:\n",
    "                print(f\"OVERFITTED AT EPOCH {epoch}\")\n",
    "                break\n",
    "    else: # if not broken with val decrease excision\n",
    "        total_epochs += 1\n",
    "        continue\n",
    "    break\n",
    "[plt.plot(accuracies, label=name) for name, accuracies in accuracy.items()]\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c33b8",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3f61aaf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m data_scoring \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m paths \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m):   \n\u001b[1;32m      4\u001b[0m     paths\u001b[38;5;241m.\u001b[39mappend(path) \n\u001b[1;32m      5\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, path))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_data'"
     ]
    }
   ],
   "source": [
    "data_scoring = []\n",
    "paths = []\n",
    "for path in tqdm.tqdm(os.listdir(\"test_data\")):   \n",
    "    paths.append(path) \n",
    "    df = pd.read_csv(os.path.join(\"test_data\", path))\n",
    "    data_scoring.append(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938a10ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting..\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [3, 1, 12, 12], expected input[1, 2870, 128, 200] to have 1 channels, but got 2870 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[313], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 6\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_scoring\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m      8\u001b[0m submission \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFilename\u001b[39m\u001b[38;5;124m\"\u001b[39m: paths, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: results\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)})\n\u001b[1;32m      9\u001b[0m submission \u001b[38;5;241m=\u001b[39m submission\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFilename\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Repositories/ai-challenge-anticartavost-/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Repositories/ai-challenge-anticartavost-/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[305], line 51\u001b[0m, in \u001b[0;36mMelConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 51\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n\u001b[1;32m     54\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool2(x)\n",
      "File \u001b[0;32m~/Desktop/Repositories/ai-challenge-anticartavost-/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Repositories/ai-challenge-anticartavost-/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Repositories/ai-challenge-anticartavost-/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Repositories/ai-challenge-anticartavost-/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [3, 1, 12, 12], expected input[1, 2870, 128, 200] to have 1 channels, but got 2870 channels instead"
     ]
    }
   ],
   "source": [
    "data_scoring = torch.tensor(data_scoring, dtype=torch.float32).to(device)\n",
    "sample_submission = pd.read_csv(\"test (1).csv\")\n",
    "print(\"predicting..\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    results = model(data_scoring) >= 0.5\n",
    "\n",
    "submission = pd.DataFrame({\"Filename\": paths, \"Label\": results.squeeze(1).to(\"cpu\")})\n",
    "submission = submission.sort_values(by=\"Filename\")\n",
    "sample_submission = sample_submission.sort_values(by=\"Filename\")\n",
    "sample_submission[\"Label\"] = submission[\"Label\"]\n",
    "sample_submission = sample_submission.sort_index()\n",
    "sample_submission.to_csv(\"submission.csv\", index=False, header=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
